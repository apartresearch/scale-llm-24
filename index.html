<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <title>Workshop on the Scaling Behavior of Large Language Models</title>

    <!-- Setup all meta-information like description and titles -->
    <meta
      name="description"
      content="The workshop on the scaling behavior of large language models invites researchers to submit projects that uncover scaling laws, with a specific foucs on inverse scaling laws, in large language models."
    />
    <meta
      name="keywords"
      content="Scaling laws, mechanistic interpretability, workshop"
    />
    <meta name="author" content="Scaling laws workshop 2024" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <!-- Load fonts Gothic A1 -->
    <link
      href="https://fonts.googleapis.com/css?family=Gothic+A1:400,700&display=swap"
      rel="stylesheet"
    />

    <!-- Load style.css -->
    <link rel="stylesheet" href="style.css" />
  </head>
  <body>
    <!-- Header with a background color filling approx. 300px and that has a title of the workshop and the date as a byline -->
    <header>
      <h1 class="fade-in">Workshop on the Scaling Behavior of Large Language Models</h1>
      <p class="fade-in">Date</p>
    </header>
    <!-- Content on white background with sections Overview, Schedule, Speakers and Organizing Committee -->
    <main class="fade-in">
      <section class="markdown">
## Introduction
        
Neural Language Models generally seem to adhere
to positive scaling laws: increasing the model size
(in terms of the number of parameters, training data
and training compute) predictably reduces tokenprediction
training loss, which in turn is a good predictor
of performance in many downstream tasks
(Kaplan et al., 2020; Hernandez et al., 2021; Hoffmann
et al., 2022; Nijkamp et al., 2022). This
scaling behavior has lead to Large Language Models
(LLMs) becoming the dominant paradigm in
Natural Language Processing, prompting speculations
about all tasks becoming eventually solved
by sufficiently large models. Sutton (2019) has
called this the "Bitter Lesson" of AI research: simple
models at scale tend to outperform attempts to
implement more complex algorithms that reflect
mental models of how we solve problems.

However, not all tasks follow these scaling
laws. Some show Inverse Scaling, i.e., the performance
drops with increasing model size, or a
non-monotonic pattern such as U-shaped Scaling
and Inverse U-shaped Scaling. Cases that defy
the general trend of positive scaling include several
of the BIG-Bench (Srivastava et al., 2022)
benchmark tasks (several with non-monotonic scaling
and some social bias tasks with inverse scaling,
the latter also previously reported by Parrish
et al. (2022)), unwanted memorization (Carlini
et al., 2023), incorrect Python code generation under
out-of-distribution (OOD) identifier redefinition
(Miceli Barone et al., 2023), poor OOD compositional
generalization of LLMs on a sematic parsing
task (Kim et al., 2022), truthfulness of LLMs
on an adversarial question answering dataset (Lin
et al., 2022), LLM sensitivity to changes to the
prompting format (Perez et al., 2021), etc. These
research efforts, among others, culminated in the
Inverse Scaling Prize (McKenzie et al., 2023) encouraging
the community to contribute and systematically
evaluate new tasks for different LLMs
families. This led to the discovery and characterization
of several types of tasks with inverse or
non-monotonic scaling behaviors.

We believe that studying the scaling behavior
of LLMs is of paramount importance in order to
discover the trajectory of progress in NLP and AI.
Determining whether and to what extent progress
is due to investment in data and compute instead of
traditional research (e.g., on different architectures
and paradigms) can help the scientific community
identify promising areas of research and inform the
expectations of general public and decision makers.
This is especially crucial now, when concerns
about AI safety and its impact on society are becoming
increasingly salient in the public discourse.
We notice that without a central venue dedicated to
promoting and sharing research on this topic, it has
been mainly published on arXiv and promoted on
social media, and much less commonly presented
at peer-reviewed NLP and ML conferences and
workshops. Our workshop aims to promote this
research by providing such venue, establishing a
place for the community to share their results and
discuss the state of the art in this quickly developing
field. While the original Inverse Scaling Prize
was highly successful, it had limitations stemming
from the need to be able to run automatic evaluations
on all the submitted entries hence supporting
only a handful of entry formats and evaluation
metrics. As a result, the challenge ignored generation
tasks, adaptive prompting strategies such as
Chain-of-Thought (Wei et al., 2023), Least-to-Most
(Zhou et al., 2023) or Tree-of-Thoughts (Yao et al.,
2023a), use of external tools (Yao et al., 2023b;
Schick et al., 2023; Qin et al., 2023), among others.
Moreover, the prize only considered fixed model
families and did not allow custom or fine-tuned
models. Our aim is to elicit paper submissions not
constrained to a fixed experimental format, allowing
the authors to choose the task types, model
families, prompting strategies and evaluation metrics
that they deem more appropriate. We also
encourage submissions on studies on internal aspects
of LLMs in relation with scale, such as attribution
(Voita et al., 2021; Ferrando et al., 2023),
mechanistic interpretation (Elhage et al. (2021);
Wang et al. (2022), to name a few), modularity in
computation (Csord√°s et al., 2021; Lepori et al.,
2023), as well as research on deployment-critical
metrics other than SoTA accuracy on benchmarks
(e.g. uncertainty, effectiveness of pruning (Frantar
and Alistarh, 2023; Sun et al., 2023), distillation,
quantization (Dettmers et al., 2022; Dettmers and
Zettlemoyer, 2023), etc.).

## Main Workshop Topics

The workshop will provide focused discussions
on multiple topics in the general field of Scaling
behavior of Large Language Models, including, but
not limited to the following:

1. Novel tasks that exhibit Inverse, U-shaped,
Inverse U-shaped or other types of scaling;
2. Scaling behavior of fine-tuned or purposebuilt
models, in particular in-distribution
(w.r.t. the fine-tuning dataset) vs. out-ofdistribution;
3. Scaling with adaptive prompting strategies,
e.g. allowing intermediate "reasoning" steps,
model self-critique or use of external tools;
4. Scaling w.r.t. additional dimensions, such as
the number of in-context/fine-tuning examples,
the number of "reasoning" steps, or the
intrinsic task complexity;
5. Scaling on non-English language tasks, in particular
low-resource languages, where models
might exhibit tradeoffs as high-resource language
training data overwhelms low-resource
language capabilities;
6. Scaling w.r.t. qualitative characteristics: internal
aspects (e.g. modularity, mechanistic
interpretability), calibration, uncertainty, effectiveness
of various techniques (pruning, defences
against adversarial attacks, etc.).

## Workshop Format

We accept short and long paper submissions with
no more than 4 and 8 pages, respectively, and an
optional 4-page supplementary material. We plan
to host at least one keynote talk, a few oral presentations
and poster sessions during one day, as well
as a panel discussion with 4-5 speakers.

## Invited Talk

Najoung Kim will give a keynote talk. Dr. Kim is an Assistant
Professor at Boston University and a researcher at
Google. She is one of the authors of the Inverse
Scaling Prize paper as well as other foundational
works in this field.
      </section>
      <section>
        <h2>Invited Speakers</h2>
        <div class="speakers">
          <div class="speaker">
            <img src="https://najoung.kim/assets/img/2022_pic.png" alt="Speaker" />
            <div>
              <h3><a href="https://najoung.kim/">Najoung Kim</a></h3>
              <p>Assistant Professor at Boston University</p>
            </div>
          </div>
      </section>
      <section class="markdown">
        ## Schedule

        To be decided
      </section>
      <section>
        <h2>Organizing Committee</h2>
        <div class="organizers">
          <div class="Organizer">
            <img src="https://homepages.inf.ed.ac.uk/amiceli/pic.jpg" alt="Organizer" />
            <div>
              <h3><a href="https://homepages.inf.ed.ac.uk/amiceli/">Antonio Valerio Miceli-Barone</a></h3>
            </div>
          </div>
          <div class="Organizer">
            <img src="img/fazlbarez.jpeg" alt="Organizer" />
            <div>
              <h3><a href="https://fbarez.github.io/">Fazl Barez</a></h3>
              <p>PhD Student Edinburgh/Oxford University</p>
            </div>
          </div>
        </div>
      </section>
      <section>
        <h2>Correspondence</h2>
        <p>
          <!-- Emails fazl@apartresearch.com -->
          Email: <a href="mailto:amiceli@ed.ac.uk">amiceli@ed.ac.uk</a></p>
    </main>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/marked/9.1.0/marked.min.js" integrity="sha512-4+zFvAejSGVlybiAKYyAz3KMjmbIT7I+wXgx190ZAsT19L2z8S4htBy1scR7CyP9pDKNSaolJMLedCigA/gFVg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
    <script>
      document.addEventListener("DOMContentLoaded", function() {
    // Get all the sections with the class "markdown"
    let markdownSections = document.querySelectorAll('section.markdown');

    markdownSections.forEach(section => {
        // Convert markdown content to HTML
        let convertedHTML = marked(section.textContent);
        
        // Replace section content with the converted HTML
        section.innerHTML = convertedHTML;
      console.log("Changed");
    });
});

    </script>
  </body>
</html>
